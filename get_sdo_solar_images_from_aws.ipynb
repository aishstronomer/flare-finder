{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union\n",
    "import zarr\n",
    "import s3fs\n",
    "\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sunpy.visualization.colormaps as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting configs\n",
    "\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_connection(path_to_zarr: os.path) -> s3fs.S3Map:\n",
    "    \"\"\"\n",
    "    Instantiate connection to aws for a given path `path_to_zarr`\n",
    "    \"\"\"\n",
    "    return s3fs.S3Map(\n",
    "        root=path_to_zarr,\n",
    "        s3=s3fs.S3FileSystem(anon=True),\n",
    "        # anonymous access requires no credentials\n",
    "        check=False,\n",
    "    )\n",
    "\n",
    "def load_single_aws_zarr(\n",
    "    path_to_zarr: os.path,\n",
    "    cache_max_single_size: int = None,\n",
    ") -> Union[zarr.Array, zarr.Group]:\n",
    "    \"\"\"\n",
    "    load zarr from s3 using LRU cache\n",
    "    \"\"\"\n",
    "    return zarr.open(\n",
    "        zarr.LRUStoreCache(\n",
    "            store=s3_connection(path_to_zarr),\n",
    "            max_size=cache_max_single_size,\n",
    "        ),\n",
    "        mode=\"r\",\n",
    "    )\n",
    "\n",
    "def get_single_solar_image(image_idx, path_to_zarr):\n",
    "    images_drry = da.from_array(load_single_aws_zarr(path_to_zarr)[\"171A\"])\n",
    "    image = np.array(images_drry[image_idx, :, :])\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "def get_sdo_solar_images_from_aws(\n",
    "    s3_root_for_sdoml_year_zarr,\n",
    "    desired_times,\n",
    "    sav_folder_path,\n",
    "    tolerance,\n",
    "):\n",
    "\n",
    "    # for desired_times, get closest times in the zarr file and corresponding indices:\n",
    "    #   images_zry_closest_idxs, images_closest_times\n",
    "    images_171a_zarray = load_single_aws_zarr(\n",
    "        path_to_zarr=s3_root_for_sdoml_year_zarr,\n",
    "    )[\"171A\"]\n",
    "    images_zry_closest_idxs = []\n",
    "    # images_zry_times = pd.to_datetime(np.array(images_171a_zarray.attrs[\"T_OBS\"]))\n",
    "\n",
    "    # TEMP: pick up images_zry_times from local\n",
    "    # pickle.dump(images_zry_times, open('temp_images_zry_times.pkl', 'wb'))\n",
    "    images_zry_times = pickle.load(open('temp_images_zry_times.pkl', 'rb'))\n",
    "\n",
    "    for selected_time in desired_times[None:None]:\n",
    "        images_zry_closest_idxs.append(np.argmin(abs(images_zry_times - selected_time)))\n",
    "    images_zry_closest_idxs = sorted(set(images_zry_closest_idxs))\n",
    "    images_closest_times = images_zry_times[images_zry_closest_idxs]\n",
    "\n",
    "    # get the image_times that have been processed already: images_processed_times\n",
    "    images_png_folder = sav_folder_path\n",
    "    images_processed_paths = glob.glob(os.path.join(images_png_folder, \"*.png\"))\n",
    "    images_processed_times = [\n",
    "        pd.to_datetime(re.sub(\".png\", \"\", os.path.basename(path)))\n",
    "        for path in images_processed_paths\n",
    "    ]\n",
    "\n",
    "    # fetch images\n",
    "    fetched_images_paths = []\n",
    "    for image_time in images_closest_times[None:None]:\n",
    "        current_img_time = image_time\n",
    "\n",
    "        # get the position of image_time in images_closest_times\n",
    "        image_time_idx = list(images_closest_times).index(image_time)\n",
    "\n",
    "        # check if the images_processed_times contains the row currently being processed and skip iter if true\n",
    "        if current_img_time in images_processed_times:\n",
    "            # print('current_img_time:', current_img_time, 'images_processed_times:', images_processed_times)\n",
    "            print(\n",
    "                f\"Skipping image_time_idx {image_time_idx} as it has been processed already.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # get current image\n",
    "        image_arr = get_single_solar_image(images_zry_closest_idxs[image_time_idx], s3_root_for_sdoml_year_zarr)\n",
    "        downsampled_pxl_posns = np.arange(0, image_arr.shape[0], 2)\n",
    "        image_arr = image_arr[downsampled_pxl_posns, :][:, downsampled_pxl_posns]\n",
    "\n",
    "        # Save the image\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(image_arr, origin=\"lower\", vmin=10, vmax=1000, cmap=plt.get_cmap(\"sdoaia171\"))\n",
    "        image_path = f\"{images_png_folder}/{current_img_time}.png\"\n",
    "        plt.savefig(image_path)\n",
    "        plt.close(\"all\")  # Close the figure manually to release resources\n",
    "\n",
    "        print(\n",
    "            f\"fetched image_time_idx: {image_time_idx} of {len(images_closest_times)}\"\n",
    "        )\n",
    "\n",
    "        fetched_images_paths.append(image_path)\n",
    "\n",
    "    return fetched_images_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m desired_times \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mdate_range(\n\u001b[1;32m      5\u001b[0m     start\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2015-01-01 00:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2015-12-31 23:59:59\u001b[39m\u001b[38;5;124m\"\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m60T\u001b[39m\u001b[38;5;124m\"\u001b[39m, tz\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTC\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m sav_folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/aishsk6/gd_to_be_archived_big_files/sdo_image_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mget_sdo_solar_images_from_aws\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_root_for_sdoml_year_zarr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesired_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msav_folder_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 27\u001b[0m, in \u001b[0;36mget_sdo_solar_images_from_aws\u001b[0;34m(s3_root_for_sdoml_year_zarr, desired_times, sav_folder_path, tolerance)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28minput\u001b[39m()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m selected_time \u001b[38;5;129;01min\u001b[39;00m desired_times[\u001b[38;5;28;01mNone\u001b[39;00m:\u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m---> 27\u001b[0m     images_zry_closest_idxs\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39margmin(\u001b[38;5;28mabs\u001b[39m(\u001b[43mimages_zry_times\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mselected_time\u001b[49m)))\n\u001b[1;32m     28\u001b[0m images_zry_closest_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(images_zry_closest_idxs))\n\u001b[1;32m     29\u001b[0m images_closest_times \u001b[38;5;241m=\u001b[39m images_zry_times[images_zry_closest_idxs]\n",
      "File \u001b[0;32m~/anaconda3/envs/mlproject/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlproject/lib/python3.12/site-packages/pandas/core/arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlproject/lib/python3.12/site-packages/pandas/core/indexes/base.py:7166\u001b[0m, in \u001b[0;36mIndex._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   7156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   7157\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(other, Index)\n\u001b[1;32m   7158\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_object_dtype(other\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   7162\u001b[0m     \u001b[38;5;66;03m# a chance to implement ops before we unwrap them.\u001b[39;00m\n\u001b[1;32m   7163\u001b[0m     \u001b[38;5;66;03m# See https://github.com/pandas-dev/pandas/issues/31109\u001b[39;00m\n\u001b[1;32m   7164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m-> 7166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlproject/lib/python3.12/site-packages/pandas/core/base.py:1381\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1381\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlproject/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:275\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# NB: We assume that extract_array and ensure_wrapped_if_datetimelike\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m#  have already been called on `left` and `right`,\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m#  and `maybe_prepare_scalar_for_op` has already been called on `right`\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# We need to special-case datetime64/timedelta64 dtypes (e.g. because numpy\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# casts integer dtypes to timedelta64 when operating with timedelta64 - GH#22390)\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    269\u001b[0m     should_extension_dispatch(left, right)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, (Timedelta, BaseOffset, Timestamp))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# Timedelta/Timestamp and other custom scalars are included in the check\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# because numexpr will fail on it, see GH#31457\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# TODO we should handle EAs consistently and move this check before the if/else\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# (https://github.com/pandas-dev/pandas/issues/41165)\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# error: Argument 2 to \"_bool_arith_check\" has incompatible type\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlproject/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlproject/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:1388\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__radd__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m   1385\u001b[0m     \u001b[38;5;66;03m# alias for __add__\u001b[39;00m\n\u001b[1;32m   1386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__add__\u001b[39m(other)\n\u001b[0;32m-> 1388\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m   1390\u001b[0m     other_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(other, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1391\u001b[0m     other \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(other)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s3_root_for_sdoml_year_zarr = (\n",
    "    \"s3://gov-nasa-hdrl-data1/contrib/fdl-sdoml/fdl-sdoml-v2/sdomlv2.zarr/2015/\"\n",
    ")\n",
    "desired_times = pd.date_range(\n",
    "    start=\"2015-01-01 00:00:00\", end=\"2015-12-31 23:59:59\", freq=\"60T\", tz=\"UTC\"\n",
    ")\n",
    "sav_folder_path = \"/Users/aishsk6/gd_to_be_archived_big_files/sdo_image_data\"\n",
    "\n",
    "get_sdo_solar_images_from_aws(\n",
    "    s3_root_for_sdoml_year_zarr,\n",
    "    desired_times,\n",
    "    sav_folder_path,\n",
    "    tolerance=pd.Timedelta(days=1000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2000-01-01', '2005-01-01'], dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "times = pd.to_datetime([\"2000\", \"2005\"])\n",
    "pickle.dump(times, open('images_zry_times.pkl', 'wb'))\n",
    "\n",
    "times = pickle.load(open('images_zry_times.pkl', 'rb'))\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the goes flare events\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "goes_events_data = pd.read_csv('goes_events_clean_2015.csv')\n",
    "\n",
    "# Filter the original dataframe for rows where 'Particulars_a' starts with 'M' or 'X'\n",
    "goes_MX_events = goes_events_data[goes_events_data['Particulars_a'].str.startswith(('M', 'X'))]\n",
    "\n",
    "goes_MX_event_times = pd.to_datetime(goes_MX_events['max_datetime'].fillna(\n",
    "    goes_MX_events['begin_datetime']), utc=True).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping image_time_idx 0 as it has been processed already.\n",
      "Skipping image_time_idx 1 as it has been processed already.\n",
      "Skipping image_time_idx 2 as it has been processed already.\n",
      "Skipping image_time_idx 3 as it has been processed already.\n",
      "Skipping image_time_idx 4 as it has been processed already.\n",
      "Skipping image_time_idx 5 as it has been processed already.\n",
      "Skipping image_time_idx 6 as it has been processed already.\n",
      "Skipping image_time_idx 7 as it has been processed already.\n",
      "Skipping image_time_idx 8 as it has been processed already.\n",
      "Skipping image_time_idx 9 as it has been processed already.\n",
      "Skipping image_time_idx 10 as it has been processed already.\n",
      "Skipping image_time_idx 11 as it has been processed already.\n",
      "Skipping image_time_idx 12 as it has been processed already.\n",
      "Skipping image_time_idx 13 as it has been processed already.\n",
      "Skipping image_time_idx 14 as it has been processed already.\n",
      "Skipping image_time_idx 15 as it has been processed already.\n",
      "Skipping image_time_idx 16 as it has been processed already.\n",
      "Skipping image_time_idx 17 as it has been processed already.\n",
      "Skipping image_time_idx 18 as it has been processed already.\n",
      "Skipping image_time_idx 19 as it has been processed already.\n",
      "Skipping image_time_idx 20 as it has been processed already.\n",
      "Skipping image_time_idx 21 as it has been processed already.\n",
      "Skipping image_time_idx 22 as it has been processed already.\n",
      "Skipping image_time_idx 23 as it has been processed already.\n",
      "Skipping image_time_idx 24 as it has been processed already.\n",
      "Skipping image_time_idx 25 as it has been processed already.\n",
      "Skipping image_time_idx 26 as it has been processed already.\n",
      "Skipping image_time_idx 27 as it has been processed already.\n",
      "Skipping image_time_idx 28 as it has been processed already.\n",
      "Skipping image_time_idx 29 as it has been processed already.\n",
      "Skipping image_time_idx 30 as it has been processed already.\n",
      "Skipping image_time_idx 31 as it has been processed already.\n",
      "Skipping image_time_idx 32 as it has been processed already.\n",
      "Skipping image_time_idx 33 as it has been processed already.\n",
      "Skipping image_time_idx 34 as it has been processed already.\n",
      "Skipping image_time_idx 35 as it has been processed already.\n",
      "Skipping image_time_idx 36 as it has been processed already.\n",
      "Skipping image_time_idx 37 as it has been processed already.\n",
      "Skipping image_time_idx 38 as it has been processed already.\n",
      "Skipping image_time_idx 39 as it has been processed already.\n",
      "Skipping image_time_idx 40 as it has been processed already.\n",
      "Skipping image_time_idx 41 as it has been processed already.\n",
      "Skipping image_time_idx 42 as it has been processed already.\n",
      "Skipping image_time_idx 43 as it has been processed already.\n",
      "Skipping image_time_idx 44 as it has been processed already.\n",
      "Skipping image_time_idx 45 as it has been processed already.\n",
      "Skipping image_time_idx 46 as it has been processed already.\n",
      "Skipping image_time_idx 47 as it has been processed already.\n",
      "Skipping image_time_idx 48 as it has been processed already.\n",
      "Skipping image_time_idx 49 as it has been processed already.\n",
      "Skipping image_time_idx 50 as it has been processed already.\n",
      "Skipping image_time_idx 51 as it has been processed already.\n",
      "Skipping image_time_idx 52 as it has been processed already.\n",
      "Skipping image_time_idx 53 as it has been processed already.\n",
      "Skipping image_time_idx 54 as it has been processed already.\n",
      "Skipping image_time_idx 55 as it has been processed already.\n",
      "Skipping image_time_idx 56 as it has been processed already.\n",
      "Skipping image_time_idx 57 as it has been processed already.\n",
      "Skipping image_time_idx 58 as it has been processed already.\n",
      "Skipping image_time_idx 59 as it has been processed already.\n",
      "Skipping image_time_idx 60 as it has been processed already.\n",
      "Skipping image_time_idx 61 as it has been processed already.\n",
      "Skipping image_time_idx 62 as it has been processed already.\n",
      "Skipping image_time_idx 63 as it has been processed already.\n",
      "Skipping image_time_idx 64 as it has been processed already.\n",
      "Skipping image_time_idx 65 as it has been processed already.\n",
      "Skipping image_time_idx 66 as it has been processed already.\n",
      "Skipping image_time_idx 67 as it has been processed already.\n",
      "Skipping image_time_idx 68 as it has been processed already.\n",
      "Skipping image_time_idx 69 as it has been processed already.\n",
      "Skipping image_time_idx 70 as it has been processed already.\n",
      "Skipping image_time_idx 71 as it has been processed already.\n",
      "Skipping image_time_idx 72 as it has been processed already.\n",
      "Skipping image_time_idx 73 as it has been processed already.\n",
      "Skipping image_time_idx 74 as it has been processed already.\n",
      "Skipping image_time_idx 75 as it has been processed already.\n",
      "Skipping image_time_idx 76 as it has been processed already.\n",
      "Skipping image_time_idx 77 as it has been processed already.\n",
      "Skipping image_time_idx 78 as it has been processed already.\n",
      "Skipping image_time_idx 79 as it has been processed already.\n",
      "Skipping image_time_idx 80 as it has been processed already.\n",
      "Skipping image_time_idx 81 as it has been processed already.\n",
      "Skipping image_time_idx 82 as it has been processed already.\n",
      "Skipping image_time_idx 83 as it has been processed already.\n",
      "Skipping image_time_idx 84 as it has been processed already.\n",
      "Skipping image_time_idx 85 as it has been processed already.\n",
      "Skipping image_time_idx 86 as it has been processed already.\n",
      "Skipping image_time_idx 87 as it has been processed already.\n",
      "Skipping image_time_idx 88 as it has been processed already.\n",
      "Skipping image_time_idx 89 as it has been processed already.\n",
      "Skipping image_time_idx 90 as it has been processed already.\n",
      "Skipping image_time_idx 91 as it has been processed already.\n",
      "Skipping image_time_idx 92 as it has been processed already.\n",
      "Skipping image_time_idx 93 as it has been processed already.\n",
      "Skipping image_time_idx 94 as it has been processed already.\n",
      "Skipping image_time_idx 95 as it has been processed already.\n",
      "Skipping image_time_idx 96 as it has been processed already.\n",
      "Skipping image_time_idx 97 as it has been processed already.\n",
      "Skipping image_time_idx 98 as it has been processed already.\n",
      "Skipping image_time_idx 99 as it has been processed already.\n",
      "Skipping image_time_idx 100 as it has been processed already.\n",
      "Skipping image_time_idx 101 as it has been processed already.\n",
      "Skipping image_time_idx 102 as it has been processed already.\n",
      "Skipping image_time_idx 103 as it has been processed already.\n",
      "Skipping image_time_idx 104 as it has been processed already.\n",
      "Skipping image_time_idx 105 as it has been processed already.\n",
      "Skipping image_time_idx 106 as it has been processed already.\n",
      "Skipping image_time_idx 107 as it has been processed already.\n",
      "Skipping image_time_idx 108 as it has been processed already.\n",
      "Skipping image_time_idx 109 as it has been processed already.\n",
      "Skipping image_time_idx 110 as it has been processed already.\n",
      "Skipping image_time_idx 111 as it has been processed already.\n",
      "Skipping image_time_idx 112 as it has been processed already.\n",
      "Skipping image_time_idx 113 as it has been processed already.\n",
      "Skipping image_time_idx 114 as it has been processed already.\n",
      "Skipping image_time_idx 115 as it has been processed already.\n",
      "Skipping image_time_idx 116 as it has been processed already.\n",
      "Skipping image_time_idx 117 as it has been processed already.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_root_for_sdoml_year_zarr = (\n",
    "    \"s3://gov-nasa-hdrl-data1/contrib/fdl-sdoml/fdl-sdoml-v2/sdomlv2.zarr/2015/\"\n",
    ")\n",
    "desired_times = goes_MX_event_times\n",
    "\n",
    "sav_folder_path = (\n",
    "    \"/Users/aishsk6/gd_to_be_archived_big_files/sdo_image_data_goes_events/\"\n",
    ")\n",
    "\n",
    "get_sdo_solar_images_from_aws(\n",
    "    s3_root_for_sdoml_year_zarr,\n",
    "    desired_times,\n",
    "    sav_folder_path,\n",
    "    tolerance=pd.Timedelta(minutes=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
