{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishstronomer/flare-finder/blob/main/flare_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xZ5d83BIX0Jl",
        "outputId": "f6effd90-81b7-457f-98fc-885f43c3cc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (2023.8.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (2024.6.0)\n",
            "Requirement already satisfied: sunpy in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (5.1.3)\n",
            "Requirement already satisfied: zarr in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6)) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (2024.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (24.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (7.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.25.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (2.13.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (3.9.5)\n",
            "Requirement already satisfied: astropy!=5.1.0,>=5.0.6 in /usr/local/lib/python3.10/dist-packages (from sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (5.3.4)\n",
            "Requirement already satisfied: parfive[ftp]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (2.1.0)\n",
            "Requirement already satisfied: pyerfa in /usr/local/lib/python3.10/dist-packages (from sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (2.0.1.4)\n",
            "Requirement already satisfied: asciitree in /usr/local/lib/python3.10/dist-packages (from zarr->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6)) (0.3.3)\n",
            "Requirement already satisfied: numcodecs>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from zarr->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fasteners in /usr/local/lib/python3.10/dist-packages (from zarr->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6)) (0.19)\n",
            "Requirement already satisfied: botocore<1.34.107,>=1.34.70 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.34.106)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.14.1)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (3.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from parfive[ftp]>=2.0.0->sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (4.66.4)\n",
            "Requirement already satisfied: aioftp>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from parfive[ftp]>=2.0.0->sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (0.22.3)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (3.7)\n"
          ]
        }
      ],
      "source": [
        "# set globals\n",
        "\n",
        "# do Google Colab things\n",
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# install dependencies\n",
        "path_to_coderepo = (\n",
        "    \"/content/drive/MyDrive/ML_project/code_repo/flare-finder\" if IN_COLAB else \".\"\n",
        ")\n",
        "if IN_COLAB:\n",
        "    !pip install -r {path_to_coderepo}/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A-aj79jI1cYr"
      },
      "outputs": [],
      "source": [
        "# set globals\n",
        "\n",
        "# import standard libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DNmcTVnQ0MXA",
        "outputId": "71e91716-c225-4d06-99bc-6271bfd9f349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets distribution: {0: 32, 1: 5}\n",
            "Predictions distribution: {0: 17, 1: 20}\n",
            "Epoch: 0, train_loss: 0.41, val_metrics: {'accuracy': 0.32, 'f1': 0.0, 'precision_class_1': 0.0, 'recall_class_1': 0.0}\n",
            "Targets distribution: {0: 32, 1: 5}\n",
            "Predictions distribution: {0: 36, 1: 1}\n",
            "Epoch: 1, train_loss: 0.24, val_metrics: {'accuracy': 0.84, 'f1': 0.0, 'precision_class_1': 0.0, 'recall_class_1': 0.0}\n",
            "Targets distribution: {0: 32, 1: 5}\n",
            "Predictions distribution: {0: 37}\n",
            "Epoch: 2, train_loss: 0.19, val_metrics: {'accuracy': 0.86, 'f1': 0.0, 'precision_class_1': 0.0, 'recall_class_1': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets distribution: {0: 32, 1: 5}\n",
            "Predictions distribution: {0: 36, 1: 1}\n",
            "Epoch: 3, train_loss: 0.24, val_metrics: {'accuracy': 0.89, 'f1': 0.33, 'precision_class_1': 1.0, 'recall_class_1': 0.2}\n",
            "Targets distribution: {0: 32, 1: 5}\n",
            "Predictions distribution: {0: 37}\n",
            "Epoch: 4, train_loss: 0.14, val_metrics: {'accuracy': 0.86, 'f1': 0.0, 'precision_class_1': 0.0, 'recall_class_1': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets distribution: {0: 32, 1: 5}\n",
            "Predictions distribution: {0: 36, 1: 1}\n",
            "Epoch: 5, train_loss: 0.09, val_metrics: {'accuracy': 0.89, 'f1': 0.33, 'precision_class_1': 1.0, 'recall_class_1': 0.2}\n",
            "Targets distribution: {0: 32, 1: 5}\n",
            "Predictions distribution: {0: 36, 1: 1}\n",
            "Epoch: 6, train_loss: 0.08, val_metrics: {'accuracy': 0.89, 'f1': 0.33, 'precision_class_1': 1.0, 'recall_class_1': 0.2}\n",
            "Targets distribution: {0: 32, 1: 5}\n",
            "Predictions distribution: {0: 36, 1: 1}\n",
            "Epoch: 7, train_loss: 0.05, val_metrics: {'accuracy': 0.89, 'f1': 0.33, 'precision_class_1': 1.0, 'recall_class_1': 0.2}\n",
            "Targets distribution: {0: 32, 1: 5}\n",
            "Predictions distribution: {0: 35, 1: 2}\n",
            "Epoch: 8, train_loss: 0.06, val_metrics: {'accuracy': 0.92, 'f1': 0.57, 'precision_class_1': 1.0, 'recall_class_1': 0.4}\n",
            "Targets distribution: {0: 32, 1: 5}\n",
            "Predictions distribution: {0: 36, 1: 1}\n",
            "Epoch: 9, train_loss: 0.06, val_metrics: {'accuracy': 0.89, 'f1': 0.33, 'precision_class_1': 1.0, 'recall_class_1': 0.2}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d2d389c939d7>\u001b[0m in \u001b[0;36m<cell line: 220>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;31m# make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m \u001b[0mpred_image_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbig_flare_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0mpred_image_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d2d389c939d7>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, image_paths)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "# creating model class for big flare prediction\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "class BigFlareFinder:\n",
        "    def __init__(self):\n",
        "        self.pytorch_model = None\n",
        "\n",
        "    def fit(self, image_paths, image_labels, val_frac=0.2):\n",
        "\n",
        "        # split the data into train-validation using sklearn and use stratified sampling\n",
        "        image_paths_train, image_paths_val, image_labels_train, image_labels_val = train_test_split(\n",
        "            image_paths, image_labels, test_size=val_frac, random_state=42, stratify=image_labels)\n",
        "\n",
        "        # for training, augment minority-class by making copies\n",
        "        image_label_counts = pd.Series(image_labels).value_counts().sort_values()\n",
        "        minority_class, majority_class = tuple(image_label_counts.index)\n",
        "        class_count_diff = image_label_counts[majority_class] - image_label_counts[minority_class]\n",
        "        image_paths_train_new = pd.Series(image_paths_train[image_labels_train == minority_class]).sample(\n",
        "            class_count_diff, replace=True, random_state=42).to_list()\n",
        "        image_labels_train_new = [minority_class for _ in range(class_count_diff)]\n",
        "        image_paths_train = image_paths_train + image_paths_train_new\n",
        "        image_labels_train = image_labels_train + image_labels_train_new\n",
        "\n",
        "        # get dataloader for train and validation data\n",
        "        train_loader = BigFlareFinder.preprocess(image_paths_train, image_labels_train)\n",
        "        validation_loader = BigFlareFinder.preprocess(image_paths_val, image_labels_val)\n",
        "\n",
        "        # fit pytorch model using dataloader\n",
        "\n",
        "        # load resnet18 model\n",
        "        model_resnet18 = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
        "\n",
        "        # Freeze all params except the BatchNorm layers, as here they are trained to the\n",
        "        # mean and standard deviation of ImageNet and we may lose some signal\n",
        "        for name, param in model_resnet18.named_parameters():\n",
        "            if(\"bn\" not in name):\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # reduce number of output classes in model\n",
        "        num_classes = 2\n",
        "        model_resnet18.fc = nn.Sequential(nn.Linear(model_resnet18.fc.in_features,512),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "\n",
        "        model_resnet18.to(device)\n",
        "        optimizer = optim.Adam(model_resnet18.parameters(), lr=0.001)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "        epochs = 10\n",
        "        device = device\n",
        "        target_class = 1\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            training_loss = 0.0\n",
        "            valid_loss = 0.0\n",
        "            model_resnet18.train()\n",
        "            for batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                inputs, targets = batch\n",
        "                targets = targets.type(torch.LongTensor)\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                output = model_resnet18(inputs)\n",
        "                loss = loss_fn(output, targets)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                training_loss += loss.data.item() * inputs.size(0)\n",
        "            training_loss /= len(train_loader.dataset)\n",
        "\n",
        "            model_resnet18.eval()\n",
        "            all_targets = []\n",
        "            all_predictions = []\n",
        "\n",
        "            for batch in validation_loader:\n",
        "                inputs, targets = batch\n",
        "                targets = targets.type(torch.LongTensor)\n",
        "                inputs = inputs.to(device)\n",
        "                output = model_resnet18(inputs)\n",
        "                targets = targets.to(device)\n",
        "                loss = loss_fn(output,targets)\n",
        "\n",
        "                valid_loss += loss.data.item() * inputs.size(0)\n",
        "                predictions = torch.max(F.softmax(output, dim=1), dim=1)[1]\n",
        "                correct = torch.eq(predictions, targets).view(-1)\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "            valid_loss /= len(validation_loader.dataset)\n",
        "\n",
        "            # Debug statements\n",
        "            print(f\"Targets distribution: {dict(zip(*np.unique(all_targets, return_counts=True)))}\")\n",
        "            print(f\"Predictions distribution: {dict(zip(*np.unique(all_predictions, return_counts=True)))}\")\n",
        "\n",
        "            print(\n",
        "                f'Epoch: {epoch}, train_loss: {round(training_loss, 2)}'\n",
        "                f', val_metrics: {BigFlareFinder.get_model_performance_metrics(all_targets, all_predictions)}')\n",
        "\n",
        "        # TODO: train on the val data (which was excluded from training earlier)\n",
        "        # - augment the val data\n",
        "        # - get a new val loader\n",
        "        # - train on the val loader\n",
        "\n",
        "        self.pytorch_model = model_resnet18\n",
        "\n",
        "    def predict(self, image_paths):\n",
        "        self.pytorch_model.eval()\n",
        "        pred_image_labels = [None]*len(image_paths)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return pred_image_labels\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model_performance_metrics(y_true, y_pred):\n",
        "        metrics_dict = {\n",
        "            \"accuracy\": round(accuracy_score(y_true, y_pred), 2),\n",
        "            \"f1\": round(f1_score(y_true, y_pred), 2),\n",
        "            \"precision_class_1\": round(precision_score(y_true, y_pred), 2),\n",
        "            \"recall_class_1\": round(recall_score(y_true, y_pred), 2),\n",
        "        }\n",
        "        return metrics_dict\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(image_paths, image_labels):\n",
        "        # make dataset of image_paths and image_labels\n",
        "        image_dimension = 224\n",
        "        image_transforms = transforms.Compose([\n",
        "            transforms.Resize((image_dimension, image_dimension)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
        "            ])\n",
        "        dataset = CustomImageDataset(image_paths, image_labels, image_transforms)\n",
        "\n",
        "        # make dataloader for dataset\n",
        "        batch_size = 32\n",
        "        num_workers = 2\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "\n",
        "# run the model on some data\n",
        "\n",
        "# get image paths and labels\n",
        "notbigflare_max_count = 200\n",
        "bigflare_max_count = 30\n",
        "image_folder_path = f\"{path_to_coderepo}/../../data/sdo_images\"\n",
        "solar_image_path_df = pd.read_csv(f\"{path_to_coderepo}/big_flare_labels.csv\").dropna()\n",
        "solar_image_path_df = pd.concat(\n",
        "    [\n",
        "        solar_image_path_df[solar_image_path_df[\"is_big_flare\"] == 0][0:notbigflare_max_count],\n",
        "        solar_image_path_df[solar_image_path_df[\"is_big_flare\"] == 1][0:bigflare_max_count],\n",
        "    ]\n",
        ")\n",
        "image_paths = (image_folder_path + '/' + solar_image_path_df['solar_image_filename']).to_list()\n",
        "image_labels = solar_image_path_df['is_big_flare'].to_list()\n",
        "\n",
        "# split data into train-test using sklearn and use stratified sampling\n",
        "image_paths_train, image_paths_test, image_labels_train, image_labels_test = train_test_split(\n",
        "    image_paths, image_labels, test_size=0.2, random_state=42, stratify=image_labels\n",
        ")\n",
        "\n",
        "# train model\n",
        "big_flare_finder = BigFlareFinder()\n",
        "model = big_flare_finder.fit(image_paths_train, image_labels_train)\n",
        "\n",
        "# make predictions\n",
        "pred_image_labels = big_flare_finder.predict(image_paths_test)\n",
        "pred_image_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dunZPQke0MXB",
        "outputId": "3a0c9d4a-7a9e-479c-da9f-6c5b463ed823"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "is_big_flare\n",
              "0.0    6109\n",
              "NaN    2323\n",
              "1.0     118\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test[\"is_big_flare\"].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2k7AsMIf5hz"
      },
      "outputs": [],
      "source": [
        "import os, shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-paKGyugBOl"
      },
      "outputs": [],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sbM4RjXgGfL"
      },
      "outputs": [],
      "source": [
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSHaKbNsatfZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path_to_data = '/content/drive/My Drive/ML_project/data/sdo_images/'\n",
        "big_flare_labels_filename = 'big_flare_labels.csv'\n",
        "big_flare_labels_filpepath = os.path.join(path_to_data, big_flare_labels_filename)\n",
        "solar_image_path_df = pd.read_csv(big_flare_labels_filpepath)\n",
        "big_flare_paths = solar_image_path_df[solar_image_path_df['is_big_flare'] == 1]['solar_image_filename'].apply(lambda x: os.path.join(path_to_data, x)).tolist()\n",
        "not_big_flare_paths = solar_image_path_df[solar_image_path_df['is_big_flare'] == 0]['solar_image_filename'].apply(lambda x: os.path.join(path_to_data, x)).tolist()\n",
        "\n",
        "len(big_flare_paths), len(not_big_flare_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uBlMZTXteCV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhbq0eCQCiJX"
      },
      "outputs": [],
      "source": [
        "model_resnet18 = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
        "model_resnet34 = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G04jSIbapqB7"
      },
      "outputs": [],
      "source": [
        "# Freeze all params except the BatchNorm layers, as here they are trained to the\n",
        "# mean and standard deviation of ImageNet and we may lose some signal\n",
        "for name, param in model_resnet18.named_parameters():\n",
        "    if(\"bn\" not in name):\n",
        "        param.requires_grad = False\n",
        "\n",
        "for name, param in model_resnet34.named_parameters():\n",
        "    if(\"bn\" not in name):\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tCK34rmprqJ"
      },
      "outputs": [],
      "source": [
        "# Replace the classifier\n",
        "num_classes = 2\n",
        "\n",
        "model_resnet18.fc = nn.Sequential(nn.Linear(model_resnet18.fc.in_features,512),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Dropout(),\n",
        "                                  nn.Linear(512, num_classes))\n",
        "\n",
        "model_resnet34.fc = nn.Sequential(nn.Linear(model_resnet34.fc.in_features,512),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Dropout(),\n",
        "                                  nn.Linear(512, num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YHzSGDoqYMZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=5, device=\"cpu\", target_class=1):\n",
        "    for epoch in range(epochs):\n",
        "        training_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output = model(inputs)\n",
        "            loss = loss_fn(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss += loss.data.item() * inputs.size(0)\n",
        "        training_loss /= len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        num_correct = 0\n",
        "        num_examples = 0\n",
        "        all_targets = []\n",
        "        all_predictions = []\n",
        "\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            output = model(inputs)\n",
        "            targets = targets.to(device)\n",
        "            loss = loss_fn(output,targets)\n",
        "            valid_loss += loss.data.item() * inputs.size(0)\n",
        "\n",
        "            predictions = torch.max(F.softmax(output, dim=1), dim=1)[1]\n",
        "            correct = torch.eq(predictions, targets).view(-1)\n",
        "            num_correct += torch.sum(correct).item()\n",
        "            num_examples += correct.shape[0]\n",
        "\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "        valid_loss /= len(val_loader.dataset)\n",
        "\n",
        "        # Calculate precision and recall for the target class\n",
        "        precision = precision_score(all_targets, all_predictions, pos_label=target_class, average='binary', zero_division=0)\n",
        "        recall = recall_score(all_targets, all_predictions, pos_label=target_class, average='binary', zero_division=0)\n",
        "\n",
        "        # Debug statements\n",
        "        print(f\"Targets distribution: {dict(zip(*np.unique(all_targets, return_counts=True)))}\")\n",
        "        print(f\"Predictions distribution: {dict(zip(*np.unique(all_predictions, return_counts=True)))}\")\n",
        "\n",
        "        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}'.format(\n",
        "            epoch, training_loss, valid_loss, num_correct / num_examples, precision, recall))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSyN5BIGtWJV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "paths_class1 = not_big_flare_paths[:70]\n",
        "paths_class2 = big_flare_paths[:30]\n",
        "split_fractions = [0.7, 0.1, 0.2]\n",
        "\n",
        "def get_dataloaders(paths_class1, paths_class2, split_fractions):\n",
        "  # split the paths list into subsets for class 1 and class 2\n",
        "  paths_class1_split = [list(subset) for subset in random_split(paths_class1, split_fractions)]\n",
        "  paths_class2_split = [list(subset) for subset in random_split(paths_class2, split_fractions)]\n",
        "\n",
        "  # defining inputs to the DataLoader function\n",
        "  img_dimensions = 224\n",
        "  img_transforms = transforms.Compose([\n",
        "      transforms.Resize((img_dimensions, img_dimensions)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
        "      ])\n",
        "  batch_size = 32\n",
        "  num_workers = 2\n",
        "\n",
        "  # function to get dataloader from class subsets\n",
        "  def get_dataloader(class1_subset, class2_subset):\n",
        "    all_subset = class1_subset + class2_subset\n",
        "    class1_subset_labels = len(class1_subset)*[0]\n",
        "    class2_subset_labels = len(class2_subset)*[1]\n",
        "    all_subset_labels = class1_subset_labels + class2_subset_labels\n",
        "    subset_dataset = CustomImageDataset(all_subset, all_subset_labels, transform=img_transforms)\n",
        "    subset_dataloader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    return subset_dataloader\n",
        "\n",
        "  data_loaders = [get_dataloader(class1_subset, class2_subset) for class1_subset, class2_subset in zip(paths_class1_split, paths_class2_split)]\n",
        "\n",
        "  return data_loaders\n",
        "\n",
        "train_data_loader, validation_data_loader, test_data_loader = get_dataloaders(paths_class1, paths_class2, split_fractions)\n",
        "train_data_loader, validation_data_loader, test_data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUZLNnqXuFVd"
      },
      "outputs": [],
      "source": [
        "print(f'Num training images: {len(train_data_loader.dataset)}')\n",
        "print(f'Num validation images: {len(validation_data_loader.dataset)}')\n",
        "print(f'Num test images: {len(test_data_loader.dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDASReG_xND1"
      },
      "source": [
        "## Train and test the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip3bkjFBpIKn"
      },
      "outputs": [],
      "source": [
        "def test_model(model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_data_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print('correct: {:d}  total: {:d}'.format(correct, total))\n",
        "    print('accuracy = {:f}'.format(correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTuZrmWFiV5r"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3ED44qupQbE"
      },
      "outputs": [],
      "source": [
        "model_resnet18.to(device)\n",
        "optimizer = optim.Adam(model_resnet18.parameters(), lr=0.001)\n",
        "train(model_resnet18, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, validation_data_loader, epochs=2, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaB7QTvDlW3o"
      },
      "source": [
        "Targets distribution: {0: 7, 1: 3} that in the validation set, there are 7 instances of class 0 and 3 instances of class 1. This tells you the actual distribution of classes in your validation data.\n",
        "\n",
        "Predictions distribution: {0: 9, 1: 1} indicates that the model predicted 9 instances as class 0 and 1 instance as class 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CVZcFCZpVUo"
      },
      "outputs": [],
      "source": [
        "test_model(model_resnet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRnMrbZ3LWAh"
      },
      "outputs": [],
      "source": [
        "model_resnet34.to(device)\n",
        "optimizer = optim.Adam(model_resnet34.parameters(), lr=0.001)\n",
        "train(model_resnet34, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, validation_data_loader, epochs=2, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZTBo5WGOt-G"
      },
      "outputs": [],
      "source": [
        "test_model(model_resnet34)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onqu63K-ldJ9"
      },
      "source": [
        "## Make some predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRT-cyxpUGWx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def find_classes(dir):\n",
        "    classes = os.listdir(dir)\n",
        "    classes.sort()\n",
        "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "    return classes, class_to_idx\n",
        "\n",
        "def make_prediction(model, filename):\n",
        "    labels, _ = find_classes('/content/drive/My Drive/ML_project/dogs-vs-cats/test')\n",
        "    img = Image.open(filename)\n",
        "    img = img_test_transforms(img)\n",
        "    img = img.unsqueeze(0)\n",
        "    prediction = model(img.to(device))\n",
        "    prediction = prediction.argmax()\n",
        "    print(labels[prediction])\n",
        "\n",
        "# make_prediction(model_resnet34, '/content/drive/My Drive/ML_project/dogs-vs-cats/test/dogs/dog.1146.jpg')\n",
        "# make_prediction(model_resnet34, '/content/drive/My Drive/ML_project/dogs-vs-cats/test/cats/cat.1226.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhu_7CTZlcOv"
      },
      "outputs": [],
      "source": [
        "torch.save(model_resnet18.state_dict(), \"./model_resnet18.pth\")\n",
        "torch.save(model_resnet34.state_dict(), \"./model_resnet34.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv1AkEtXxsBX"
      },
      "source": [
        "## Load the models from disk and test with an ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbtpycTSxn5j"
      },
      "outputs": [],
      "source": [
        "# Remember that you must call model.eval() to set dropout and batch normalization layers to\n",
        "# evaluation mode before running inference. Failing to do this will yield inconsistent inference result\n",
        "\n",
        "resnet18 = torch.hub.load('pytorch/vision', 'resnet18')\n",
        "resnet18.fc = nn.Sequential(nn.Linear(resnet18.fc.in_features,512),nn.ReLU(), nn.Dropout(), nn.Linear(512, num_classes))\n",
        "resnet18.load_state_dict(torch.load('./model_resnet18.pth'))\n",
        "resnet18.eval()\n",
        "\n",
        "resnet34 = torch.hub.load('pytorch/vision', 'resnet34')\n",
        "resnet34.fc = nn.Sequential(nn.Linear(resnet34.fc.in_features,512),nn.ReLU(), nn.Dropout(), nn.Linear(512, num_classes))\n",
        "resnet34.load_state_dict(torch.load('./model_resnet34.pth'))\n",
        "resnet34.eval()\n",
        "\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKKYcCuRxzOw"
      },
      "outputs": [],
      "source": [
        "# Test against the average of each prediction from the two models\n",
        "models_ensemble = [resnet18.to(device), resnet34.to(device)]\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_data_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        predictions = [i(images).data for i in models_ensemble]\n",
        "        avg_predictions = torch.mean(torch.stack(predictions), dim=0)\n",
        "        _, predicted = torch.max(avg_predictions, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('accuracy = {:f}'.format(correct / total))\n",
        "print('correct: {:d}  total: {:d}'.format(correct, total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZABx-YIx4Ea"
      },
      "outputs": [],
      "source": [
        "# Assuming your model and data are on the same device (e.g., 'cuda' or 'cpu')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet34.to(device)\n",
        "\n",
        "# Example usage\n",
        "make_prediction(resnet34, '/content/drive/My Drive/ML_project/dogs-vs-cats/test/dogs/dog.1146.jpg')\n",
        "make_prediction(resnet34, '/content/drive/My Drive/ML_project/dogs-vs-cats/test/cats/cat.1226.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZJMyJu0zhEu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}