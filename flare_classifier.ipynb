{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishstronomer/flare-finder/blob/main/flare_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flares"
      ],
      "metadata": {
        "id": "xZ5d83BIX0Jl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h2k7AsMIf5hz"
      },
      "outputs": [],
      "source": [
        "import os, shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-paKGyugBOl",
        "outputId": "52a86a89-7061-4c66-be09-6c30388bb9f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sbM4RjXgGfL",
        "outputId": "5f380072-8fa1-4770-991a-7fd8a18497ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Amamas_visa_application     Home_purchase\t\t Paper_reviews\t turning_off_bitlocker.docx\n",
            " Anime_images\t\t     Industry_training\t\t Recipes\t'Untitled document.gdoc'\n",
            " catdog_with_indices.ipynb   ML_project\t\t\t Research\t Vava_maulo_art_project\n",
            "'Colab Notebooks'\t     NGIMS_gravity_waves_codes\t Research_data\n",
            " GEDI.gdoc\t\t     Paper_2\t\t\t Tax_2023\n"
          ]
        }
      ],
      "source": [
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "path_to_data = '/content/drive/My Drive/ML_project/data/sdo_images/'\n",
        "big_flare_labels_filename = 'big_flare_labels.csv'\n",
        "big_flare_labels_filpepath = os.path.join(path_to_data, big_flare_labels_filename)\n",
        "solar_image_path_df = pd.read_csv(big_flare_labels_filpepath)\n",
        "big_flare_paths = solar_image_path_df[solar_image_path_df['is_big_flare'] == 1]['solar_image_filename'].apply(lambda x: os.path.join(path_to_data, x)).tolist()\n",
        "not_big_flare_paths = solar_image_path_df[solar_image_path_df['is_big_flare'] == 0]['solar_image_filename'].apply(lambda x: os.path.join(path_to_data, x)).tolist()\n",
        "\n",
        "len(big_flare_paths), len(not_big_flare_paths)"
      ],
      "metadata": {
        "id": "DSHaKbNsatfZ",
        "outputId": "d6f14be2-0f5b-46d9-83b2-1bcecfaec31a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118, 6109)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9uBlMZTXteCV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhbq0eCQCiJX",
        "outputId": "67dfb90b-3042-469b-c769-2c687f9aeca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 117MB/s]\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 90.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "model_resnet18 = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
        "model_resnet34 = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G04jSIbapqB7"
      },
      "outputs": [],
      "source": [
        "# Freeze all params except the BatchNorm layers, as here they are trained to the\n",
        "# mean and standard deviation of ImageNet and we may lose some signal\n",
        "for name, param in model_resnet18.named_parameters():\n",
        "    if(\"bn\" not in name):\n",
        "        param.requires_grad = False\n",
        "\n",
        "for name, param in model_resnet34.named_parameters():\n",
        "    if(\"bn\" not in name):\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8tCK34rmprqJ"
      },
      "outputs": [],
      "source": [
        "# Replace the classifier\n",
        "num_classes = 2\n",
        "\n",
        "model_resnet18.fc = nn.Sequential(nn.Linear(model_resnet18.fc.in_features,512),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Dropout(),\n",
        "                                  nn.Linear(512, num_classes))\n",
        "\n",
        "model_resnet34.fc = nn.Sequential(nn.Linear(model_resnet34.fc.in_features,512),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Dropout(),\n",
        "                                  nn.Linear(512, num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "9YHzSGDoqYMZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=5, device=\"cpu\", target_class=1):\n",
        "    for epoch in range(epochs):\n",
        "        training_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output = model(inputs)\n",
        "            loss = loss_fn(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss += loss.data.item() * inputs.size(0)\n",
        "        training_loss /= len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        num_correct = 0\n",
        "        num_examples = 0\n",
        "        all_targets = []\n",
        "        all_predictions = []\n",
        "\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            output = model(inputs)\n",
        "            targets = targets.to(device)\n",
        "            loss = loss_fn(output,targets)\n",
        "            valid_loss += loss.data.item() * inputs.size(0)\n",
        "\n",
        "            predictions = torch.max(F.softmax(output, dim=1), dim=1)[1]\n",
        "            correct = torch.eq(predictions, targets).view(-1)\n",
        "            num_correct += torch.sum(correct).item()\n",
        "            num_examples += correct.shape[0]\n",
        "\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "        valid_loss /= len(val_loader.dataset)\n",
        "\n",
        "        # Calculate precision and recall for the target class\n",
        "        precision = precision_score(all_targets, all_predictions, pos_label=target_class, average='binary', zero_division=0)\n",
        "        recall = recall_score(all_targets, all_predictions, pos_label=target_class, average='binary', zero_division=0)\n",
        "\n",
        "        # Debug statements\n",
        "        print(f\"Targets distribution: {dict(zip(*np.unique(all_targets, return_counts=True)))}\")\n",
        "        print(f\"Predictions distribution: {dict(zip(*np.unique(all_predictions, return_counts=True)))}\")\n",
        "\n",
        "        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}'.format(\n",
        "            epoch, training_loss, valid_loss, num_correct / num_examples, precision, recall))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "JSyN5BIGtWJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64993a9-1093-40db-b2b0-026eafc13693"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x786470f797e0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x786470f79840>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x786470f7bf10>)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "paths_class1 = not_big_flare_paths[:70]\n",
        "paths_class2 = big_flare_paths[:30]\n",
        "split_fractions = [0.7, 0.1, 0.2]\n",
        "\n",
        "def get_dataloaders(paths_class1, paths_class2, split_fractions):\n",
        "  # split the paths list into subsets for class 1 and class 2\n",
        "  paths_class1_split = [list(subset) for subset in random_split(paths_class1, split_fractions)]\n",
        "  paths_class2_split = [list(subset) for subset in random_split(paths_class2, split_fractions)]\n",
        "\n",
        "  # defining inputs to the DataLoader function\n",
        "  img_dimensions = 224\n",
        "  img_transforms = transforms.Compose([\n",
        "      transforms.Resize((img_dimensions, img_dimensions)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
        "      ])\n",
        "  batch_size = 32\n",
        "  num_workers = 2\n",
        "\n",
        "  # function to get dataloader from class subsets\n",
        "  def get_dataloader(class1_subset, class2_subset):\n",
        "    all_subset = class1_subset + class2_subset\n",
        "    class1_subset_labels = len(class1_subset)*[0]\n",
        "    class2_subset_labels = len(class2_subset)*[1]\n",
        "    all_subset_labels = class1_subset_labels + class2_subset_labels\n",
        "    subset_dataset = CustomImageDataset(all_subset, all_subset_labels, transform=img_transforms)\n",
        "    subset_dataloader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    return subset_dataloader\n",
        "\n",
        "  data_loaders = [get_dataloader(class1_subset, class2_subset) for class1_subset, class2_subset in zip(paths_class1_split, paths_class2_split)]\n",
        "\n",
        "  return data_loaders\n",
        "\n",
        "train_data_loader, validation_data_loader, test_data_loader = get_dataloaders(paths_class1, paths_class2, split_fractions)\n",
        "train_data_loader, validation_data_loader, test_data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUZLNnqXuFVd",
        "outputId": "e7ffbe3d-9ca0-4c44-dc0b-0c24a2fb464c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num training images: 70\n",
            "Num validation images: 10\n",
            "Num test images: 20\n"
          ]
        }
      ],
      "source": [
        "print(f'Num training images: {len(train_data_loader.dataset)}')\n",
        "print(f'Num validation images: {len(validation_data_loader.dataset)}')\n",
        "print(f'Num test images: {len(test_data_loader.dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and test the models"
      ],
      "metadata": {
        "id": "XDASReG_xND1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Ip3bkjFBpIKn"
      },
      "outputs": [],
      "source": [
        "def test_model(model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_data_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print('correct: {:d}  total: {:d}'.format(correct, total))\n",
        "    print('accuracy = {:f}'.format(correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "oTuZrmWFiV5r"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3ED44qupQbE",
        "outputId": "f2af0bd7-f301-41dc-f9f9-508d91888250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets distribution: {0: 7, 1: 3}\n",
            "Predictions distribution: {0: 9, 1: 1}\n",
            "Epoch: 0, Training Loss: 0.5165, Validation Loss: 0.5316, Accuracy: 0.8000, Precision: 1.0000, Recall: 0.3333\n",
            "Targets distribution: {0: 7, 1: 3}\n",
            "Predictions distribution: {0: 9, 1: 1}\n",
            "Epoch: 1, Training Loss: 0.4226, Validation Loss: 0.5687, Accuracy: 0.8000, Precision: 1.0000, Recall: 0.3333\n"
          ]
        }
      ],
      "source": [
        "model_resnet18.to(device)\n",
        "optimizer = optim.Adam(model_resnet18.parameters(), lr=0.001)\n",
        "train(model_resnet18, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, validation_data_loader, epochs=2, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Targets distribution: {0: 7, 1: 3} that in the validation set, there are 7 instances of class 0 and 3 instances of class 1. This tells you the actual distribution of classes in your validation data.\n",
        "\n",
        "Predictions distribution: {0: 9, 1: 1} indicates that the model predicted 9 instances as class 0 and 1 instance as class 1.\n"
      ],
      "metadata": {
        "id": "yaB7QTvDlW3o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CVZcFCZpVUo",
        "outputId": "407f8198-74e1-4698-c83d-a57d22168684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct: 15  total: 20\n",
            "accuracy = 0.750000\n"
          ]
        }
      ],
      "source": [
        "test_model(model_resnet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRnMrbZ3LWAh",
        "outputId": "c53627d3-c3d8-458f-e492-811ef5fee621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets distribution: {0: 7, 1: 3}\n",
            "Predictions distribution: {0: 10}\n",
            "Epoch: 0, Training Loss: 0.5984, Validation Loss: 0.6266, Accuracy: 0.7000, Precision: 0.0000, Recall: 0.0000\n",
            "Targets distribution: {0: 7, 1: 3}\n",
            "Predictions distribution: {0: 10}\n",
            "Epoch: 1, Training Loss: 0.5204, Validation Loss: 0.6178, Accuracy: 0.7000, Precision: 0.0000, Recall: 0.0000\n"
          ]
        }
      ],
      "source": [
        "model_resnet34.to(device)\n",
        "optimizer = optim.Adam(model_resnet34.parameters(), lr=0.001)\n",
        "train(model_resnet34, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, validation_data_loader, epochs=2, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZTBo5WGOt-G",
        "outputId": "bfdf6962-f768-4e70-f8a9-16c49acac0cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct: 16  total: 20\n",
            "accuracy = 0.800000\n"
          ]
        }
      ],
      "source": [
        "test_model(model_resnet34)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onqu63K-ldJ9"
      },
      "source": [
        "## Make some predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRT-cyxpUGWx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def find_classes(dir):\n",
        "    classes = os.listdir(dir)\n",
        "    classes.sort()\n",
        "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "    return classes, class_to_idx\n",
        "\n",
        "def make_prediction(model, filename):\n",
        "    labels, _ = find_classes('/content/drive/My Drive/ML_project/dogs-vs-cats/test')\n",
        "    img = Image.open(filename)\n",
        "    img = img_test_transforms(img)\n",
        "    img = img.unsqueeze(0)\n",
        "    prediction = model(img.to(device))\n",
        "    prediction = prediction.argmax()\n",
        "    print(labels[prediction])\n",
        "\n",
        "# make_prediction(model_resnet34, '/content/drive/My Drive/ML_project/dogs-vs-cats/test/dogs/dog.1146.jpg')\n",
        "# make_prediction(model_resnet34, '/content/drive/My Drive/ML_project/dogs-vs-cats/test/cats/cat.1226.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhu_7CTZlcOv"
      },
      "outputs": [],
      "source": [
        "torch.save(model_resnet18.state_dict(), \"./model_resnet18.pth\")\n",
        "torch.save(model_resnet34.state_dict(), \"./model_resnet34.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the models from disk and test with an ensemble"
      ],
      "metadata": {
        "id": "zv1AkEtXxsBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remember that you must call model.eval() to set dropout and batch normalization layers to\n",
        "# evaluation mode before running inference. Failing to do this will yield inconsistent inference result\n",
        "\n",
        "resnet18 = torch.hub.load('pytorch/vision', 'resnet18')\n",
        "resnet18.fc = nn.Sequential(nn.Linear(resnet18.fc.in_features,512),nn.ReLU(), nn.Dropout(), nn.Linear(512, num_classes))\n",
        "resnet18.load_state_dict(torch.load('./model_resnet18.pth'))\n",
        "resnet18.eval()\n",
        "\n",
        "resnet34 = torch.hub.load('pytorch/vision', 'resnet34')\n",
        "resnet34.fc = nn.Sequential(nn.Linear(resnet34.fc.in_features,512),nn.ReLU(), nn.Dropout(), nn.Linear(512, num_classes))\n",
        "resnet34.load_state_dict(torch.load('./model_resnet34.pth'))\n",
        "resnet34.eval()\n",
        "\n",
        "print(\"done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "zbtpycTSxn5j",
        "outputId": "b8837fc5-ef8f-45fd-8b24-545b9a407e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './model_resnet18.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-44e3c9be27e3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresnet18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pytorch/vision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'resnet18'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model_resnet18.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model_resnet18.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test against the average of each prediction from the two models\n",
        "models_ensemble = [resnet18.to(device), resnet34.to(device)]\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_data_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        predictions = [i(images).data for i in models_ensemble]\n",
        "        avg_predictions = torch.mean(torch.stack(predictions), dim=0)\n",
        "        _, predicted = torch.max(avg_predictions, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('accuracy = {:f}'.format(correct / total))\n",
        "print('correct: {:d}  total: {:d}'.format(correct, total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKKYcCuRxzOw",
        "outputId": "8e12be95-fa5c-46c5-cdd4-17de538f8ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 0.984444\n",
            "correct: 443  total: 450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your model and data are on the same device (e.g., 'cuda' or 'cpu')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet34.to(device)\n",
        "\n",
        "# Example usage\n",
        "make_prediction(resnet34, '/content/drive/My Drive/ML_project/dogs-vs-cats/test/dogs/dog.1146.jpg')\n",
        "make_prediction(resnet34, '/content/drive/My Drive/ML_project/dogs-vs-cats/test/cats/cat.1226.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZABx-YIx4Ea",
        "outputId": "4d1d8d33-b12d-4855-e06e-68e268f9a41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dogs\n",
            "cats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SZJMyJu0zhEu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFiZ1ebV9dj4IsyQJWFW66",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}