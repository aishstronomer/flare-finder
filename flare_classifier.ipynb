{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishstronomer/flare-finder/blob/main/flare_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to-do:\n",
        "# -"
      ],
      "metadata": {
        "id": "TMntV5kj88Ll"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xZ5d83BIX0Jl",
        "outputId": "2bbc993b-500a-43dc-cd58-045cb310a9ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (2023.8.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (3.7.1)\n",
            "Collecting s3fs (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4))\n",
            "  Downloading s3fs-2024.6.0-py3-none-any.whl (29 kB)\n",
            "Collecting sunpy (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5))\n",
            "  Downloading sunpy-5.1.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zarr (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6))\n",
            "  Downloading zarr-2.18.2-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.2/210.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (7.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.25.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (2.8.2)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4))\n",
            "  Downloading aiobotocore-2.13.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec>=2021.09.0 (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2))\n",
            "  Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (3.9.5)\n",
            "Requirement already satisfied: astropy!=5.1.0,>=5.0.6 in /usr/local/lib/python3.10/dist-packages (from sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (5.3.4)\n",
            "Collecting parfive[ftp]>=2.0.0 (from sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5))\n",
            "  Downloading parfive-2.1.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pyerfa in /usr/local/lib/python3.10/dist-packages (from sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (2.0.1.4)\n",
            "Collecting asciitree (from zarr->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6))\n",
            "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numcodecs>=0.10.0 (from zarr->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6))\n",
            "  Downloading numcodecs-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners (from zarr->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6))\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Collecting botocore<1.34.107,>=1.34.70 (from aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4))\n",
            "  Downloading botocore-1.34.106-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.14.1)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4))\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (3.19.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from parfive[ftp]>=2.0.0->sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (4.66.4)\n",
            "Collecting aioftp>=0.17.1 (from parfive[ftp]>=2.0.0->sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5))\n",
            "  Downloading aioftp-0.22.3-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.16.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (3.7)\n",
            "Building wheels for collected packages: asciitree\n",
            "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5034 sha256=5bc725b1d8314e3bf8786b954867fa546132b16631ed847c0ae5e1c6090be99a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/4e/be/1171b40f43b918087657ec57cf3b81fa1a2e027d8755baa184\n",
            "Successfully built asciitree\n",
            "Installing collected packages: asciitree, numcodecs, jmespath, fsspec, fasteners, aioitertools, aioftp, zarr, botocore, parfive, aiobotocore, s3fs, sunpy\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.6.0\n",
            "    Uninstalling fsspec-2023.6.0:\n",
            "      Successfully uninstalled fsspec-2023.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.0+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2024.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiobotocore-2.13.0 aioftp-0.22.3 aioitertools-0.11.0 asciitree-0.3.3 botocore-1.34.106 fasteners-0.19 fsspec-2024.6.0 jmespath-1.0.1 numcodecs-0.12.1 parfive-2.1.0 s3fs-2024.6.0 sunpy-5.1.4 zarr-2.18.2\n"
          ]
        }
      ],
      "source": [
        "# set globals\n",
        "\n",
        "# do Google Colab things\n",
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# install dependencies\n",
        "path_to_coderepo = (\n",
        "    \"/content/drive/MyDrive/ML_project/code_repo/flare-finder\" if IN_COLAB else \".\"\n",
        ")\n",
        "if IN_COLAB:\n",
        "    !pip install -r {path_to_coderepo}/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNmcTVnQ0MXA",
        "outputId": "2e641e74-5bd6-432b-e108-2ef24c317efd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/ML_project/code_repo/flare-finder/../../data/sdo_images/2015-09-12 19:00:11.340000+00:00.png',\n",
              " '/content/drive/MyDrive/ML_project/code_repo/flare-finder/../../data/sdo_images/2015-09-12 20:00:11.340000+00:00.png')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# creating model class for big flare prediction\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import ResNet18_Weights\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "class BigFlareFinder:\n",
        "    def __init__(self):\n",
        "\n",
        "        # init pytorch model\n",
        "        self.pytorch_model = None\n",
        "\n",
        "        # set things to make training deterministic\n",
        "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "        torch.manual_seed(42)\n",
        "        torch.use_deterministic_algorithms(True)\n",
        "\n",
        "        # set device\n",
        "        self.device = None\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "    def fit(self, image_paths, image_labels, val_frac=0.2):\n",
        "\n",
        "        # split the data into train-validation using sklearn and use stratified sampling\n",
        "        image_paths_train, image_paths_val, image_labels_train, image_labels_val = train_test_split(\n",
        "            image_paths, image_labels, test_size=val_frac, random_state=42, stratify=image_labels)\n",
        "\n",
        "        # for training, augment minority-class by making copies\n",
        "        image_paths_train, image_labels_train = BigFlareFinder.augment_minority_class(image_paths_train, image_labels_train)\n",
        "\n",
        "        # get dataloader for train and validation data\n",
        "        train_loader = BigFlareFinder.preprocess(image_paths_train, image_labels_train)\n",
        "        validation_loader = BigFlareFinder.preprocess(image_paths_val, image_labels_val)\n",
        "\n",
        "        # fit pytorch model using dataloader\n",
        "\n",
        "        # load resnet18 model\n",
        "        # model_resnet18 = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
        "        model_resnet18 = torch.hub.load('pytorch/vision', 'resnet18', weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "        # Freeze all params except the BatchNorm layers, as here they are trained to the\n",
        "        # mean and standard deviation of ImageNet and we may lose some signal\n",
        "        for name, param in model_resnet18.named_parameters():\n",
        "            if(\"bn\" not in name):\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # reduce number of output classes in model\n",
        "        num_classes = 2\n",
        "        model_resnet18.fc = nn.Sequential(nn.Linear(model_resnet18.fc.in_features,512),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "\n",
        "\n",
        "        model_resnet18.to(self.device)\n",
        "        optimizer = optim.Adam(model_resnet18.parameters())\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "        epochs = 7 # 10\n",
        "        target_class = 1\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            training_loss = 0.0\n",
        "            valid_loss = 0.0\n",
        "            model_resnet18.train()\n",
        "            for batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                inputs, targets = batch\n",
        "                targets = targets.type(torch.LongTensor)\n",
        "                inputs = inputs.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "                output = model_resnet18(inputs)\n",
        "                loss = loss_fn(output, targets)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                training_loss += loss.data.item() * inputs.size(0)\n",
        "            training_loss /= len(train_loader.dataset)\n",
        "\n",
        "            model_resnet18.eval()\n",
        "            # all_targets = []\n",
        "            image_labels_val_pred = []\n",
        "\n",
        "            for batch in validation_loader:\n",
        "                inputs, targets = batch\n",
        "                inputs = inputs.to(self.device)\n",
        "                output = model_resnet18(inputs)\n",
        "                predictions = torch.max(F.softmax(output, dim=1), dim=1)[1]\n",
        "                image_labels_val_pred.extend(predictions.cpu().numpy())\n",
        "\n",
        "            print(\n",
        "                f\"Train epoch: {epoch}\"\n",
        "                f\", train_loss: {round(training_loss, 2)}\"\n",
        "                f\"\\nval_metrics: {BigFlareFinder.get_model_performance_metrics(image_labels_val, image_labels_val_pred)}\"\n",
        "            )\n",
        "\n",
        "        # train on the val data (which was excluded from training earlier)\n",
        "        image_paths_val, image_labels_val = BigFlareFinder.augment_minority_class(image_paths_val, image_labels_val)\n",
        "        val_loader = BigFlareFinder.preprocess(image_paths_val, image_labels_val)\n",
        "        for epoch in range(epochs):\n",
        "            for batch in val_loader:\n",
        "                optimizer.zero_grad()\n",
        "                inputs, targets = batch\n",
        "                targets = targets.type(torch.LongTensor)\n",
        "                inputs = inputs.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "                output = model_resnet18(inputs)\n",
        "                loss = loss_fn(output, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                training_loss += loss.data.item() * inputs.size(0)\n",
        "            training_loss /= len(train_loader.dataset)\n",
        "            model_resnet18.eval()\n",
        "            print(\n",
        "                f\"Train-on-val epoch: {epoch}\"\n",
        "                f\", train_loss: {round(training_loss, 2)}\"\n",
        "            )\n",
        "\n",
        "        # save trained model to self\n",
        "        self.pytorch_model = model_resnet18\n",
        "\n",
        "    @staticmethod\n",
        "    def augment_minority_class(image_paths, image_labels):\n",
        "        # init resut\n",
        "        image_paths_aug = []\n",
        "        image_labels_aug = []\n",
        "\n",
        "        # augment data\n",
        "        image_labels_counts = pd.Series(image_labels).value_counts().sort_values()\n",
        "        minority_class, majority_class = tuple(image_labels_counts.index)\n",
        "        class_count_diff = image_labels_counts[majority_class] - image_labels_counts[minority_class]\n",
        "        image_paths_new = pd.Series(image_paths[image_labels == minority_class]).sample(\n",
        "            class_count_diff, replace=True, random_state=42).to_list()\n",
        "        image_labels_new = [minority_class] * class_count_diff\n",
        "        image_paths_aug = image_paths + image_paths_new\n",
        "        image_labels_aug = image_labels + image_labels_new\n",
        "\n",
        "        # shuffle augmented data\n",
        "        image_paths_aug, image_labels_aug = zip(\n",
        "            *np.random.default_rng(seed=42).permutation(list(zip(image_paths_aug, image_labels_aug))))\n",
        "        image_paths_aug = [str(path) for path in image_paths_aug]\n",
        "        image_labels_aug = [float(label) for label in image_labels_aug]\n",
        "\n",
        "        return image_paths_aug, image_labels_aug\n",
        "\n",
        "    def predict(self, image_paths):\n",
        "\n",
        "        # init result\n",
        "        pred_labels = []\n",
        "\n",
        "        # get dataloader\n",
        "        dataloader = BigFlareFinder.preprocess(image_paths)\n",
        "\n",
        "        # make predictions\n",
        "        for batch in dataloader:\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(self.device)\n",
        "            output = self.pytorch_model(inputs)\n",
        "            predictions = torch.max(F.softmax(output, dim=1), dim=1)[1]\n",
        "            pred_labels.extend(predictions.cpu().numpy())\n",
        "\n",
        "        return pred_labels\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model_performance_metrics(y_true, y_pred):\n",
        "        metrics_dict = {\n",
        "            \"accuracy\": round(accuracy_score(y_true, y_pred), 2),\n",
        "            \"f1\": round(f1_score(y_true, y_pred), 2),\n",
        "            \"precision_class_1\": round(precision_score(y_true, y_pred, zero_division=0), 2),\n",
        "            \"recall_class_1\": round(recall_score(y_true, y_pred, zero_division=0), 2),\n",
        "            \"actual_distru\": pd.Series(y_true).value_counts().sort_index().to_dict(),\n",
        "            \"pred_distru\": pd.Series(y_pred).value_counts().sort_index().to_dict(),\n",
        "        }\n",
        "        return metrics_dict\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(image_paths, image_labels=None):\n",
        "        # make dataset of image_paths and image_labels\n",
        "        image_dimension = 224\n",
        "        image_transforms = transforms.Compose([\n",
        "            transforms.Resize((image_dimension, image_dimension)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
        "            ])\n",
        "        image_labels = image_labels or [0]*len(image_paths)\n",
        "        dataset = CustomImageDataset(image_paths, image_labels, image_transforms)\n",
        "\n",
        "        # make dataloader for dataset\n",
        "        batch_size = 32\n",
        "        num_workers = 2\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    @staticmethod\n",
        "    def split_into_train_test_by_time(image_paths, image_labels, test_frac):\n",
        "        # make images_df\n",
        "        images_df = pd.DataFrame({'image_path': image_paths, 'image_label': image_labels})\n",
        "\n",
        "        # add a datetime column\n",
        "        images_df['datetime'] = pd.to_datetime(images_df['image_path'].str.split('/').str[-1].str[None:-4])\n",
        "\n",
        "        # sort images_df by datetime\n",
        "        images_df = images_df.sort_values('datetime')\n",
        "\n",
        "        # find min and max datetimes of images_df\n",
        "        min_datetime = images_df['datetime'].min()\n",
        "        max_datetime = images_df['datetime'].max()\n",
        "\n",
        "        # find time span between min and max datetimes (in days)\n",
        "        time_span_in_days = (max_datetime - min_datetime).days\n",
        "\n",
        "        # get the two dfs by splitting the time span into the desired ratio\n",
        "        num_train_days = time_span_in_days * (1 - test_frac)\n",
        "        end_train_datetime = min_datetime + pd.to_timedelta(num_train_days, unit = 'days')\n",
        "        train_df = images_df[images_df['datetime'] <= end_train_datetime]\n",
        "        test_df = images_df[images_df['datetime'] > end_train_datetime]\n",
        "\n",
        "        # get image_paths_train, image_paths_test, image_labels_train, image_labels_test\n",
        "        image_paths_train = train_df['image_path'].to_list()\n",
        "        image_paths_test = test_df['image_path'].to_list()\n",
        "        image_labels_train = train_df['image_label'].to_list()\n",
        "        image_labels_test = test_df['image_label'].to_list()\n",
        "\n",
        "        return image_paths_train, image_paths_test, image_labels_train, image_labels_test\n",
        "\n",
        "# run the model on some data\n",
        "\n",
        "# get the image data: image_paths, image_labels\n",
        "image_folder_path = f\"{path_to_coderepo}/../../data/sdo_images\"\n",
        "big_flare_labels_df = pd.read_csv(f\"{path_to_coderepo}/big_flare_labels.csv\").dropna()\n",
        "image_paths = (image_folder_path + '/' + big_flare_labels_df['solar_image_filename']).to_list()\n",
        "image_labels = big_flare_labels_df['is_big_flare'].to_list()\n",
        "\n",
        "# split image_paths and image_labels by time: image_paths_train, image_paths_test, image_labels_train, image_labels_test\n",
        "test_frac = 0.3\n",
        "image_paths_train, image_paths_test, image_labels_train, image_labels_test = BigFlareFinder.split_into_train_test_by_time(image_paths, image_labels, test_frac)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # get train, test data\n",
        "# train_df, test_df = BigFlareFinder.split_into_train_test_by_time(big_flare_labels_df, test_frac = 0.3)\n",
        "# image_paths_train = (image_folder_path + '/' + train_df['solar_image_filename']).to_list()\n",
        "# image_labels_train = train_df['is_big_flare'].to_list()\n",
        "# image_paths_test = (image_folder_path + '/' + test_df['solar_image_filename']).to_list()\n",
        "# image_labels_test = test_df['is_big_flare'].to_list()\n",
        "\n",
        "# # train model\n",
        "# big_flare_finder = BigFlareFinder()\n",
        "# model = big_flare_finder.fit(image_paths_train, image_labels_train)\n",
        "\n",
        "# # make predictions\n",
        "# image_labels_test_pred = big_flare_finder.predict(image_paths_test)\n",
        "\n",
        "# # get pred metrics\n",
        "# test_metrics = BigFlareFinder.get_model_performance_metrics(image_labels_test, image_labels_test_pred)\n",
        "# print(f\"\\ntest_metrics: {test_metrics}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dunZPQke0MXB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}