{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishstronomer/flare-finder/blob/main/flare_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TMntV5kj88Ll"
      },
      "outputs": [],
      "source": [
        "# to-do:\n",
        "# -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xZ5d83BIX0Jl",
        "outputId": "13416904-e997-4207-ef12-2e56b85610b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (2023.8.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (2024.6.0)\n",
            "Requirement already satisfied: sunpy in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (5.1.4)\n",
            "Requirement already satisfied: zarr in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6)) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (2024.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (7.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.25.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (2.13.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (3.9.5)\n",
            "Requirement already satisfied: astropy!=5.1.0,>=5.0.6 in /usr/local/lib/python3.10/dist-packages (from sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (5.3.4)\n",
            "Requirement already satisfied: parfive[ftp]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (2.1.0)\n",
            "Requirement already satisfied: pyerfa in /usr/local/lib/python3.10/dist-packages (from sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (2.0.1.4)\n",
            "Requirement already satisfied: asciitree in /usr/local/lib/python3.10/dist-packages (from zarr->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6)) (0.3.3)\n",
            "Requirement already satisfied: numcodecs>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from zarr->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fasteners in /usr/local/lib/python3.10/dist-packages (from zarr->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 6)) (0.19)\n",
            "Requirement already satisfied: botocore<1.34.107,>=1.34.70 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.34.106)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.14.1)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (3.19.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from parfive[ftp]>=2.0.0->sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (4.66.4)\n",
            "Requirement already satisfied: aioftp>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from parfive[ftp]>=2.0.0->sunpy->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 5)) (0.22.3)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->-r /content/drive/MyDrive/ML_project/code_repo/flare-finder/requirements.txt (line 4)) (3.7)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 856, in _parseNoCache\n",
            "    tokens = fn(instring, tokens_start, ret_tokens)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 291, in wrapper\n",
            "    ret = func(*args[limit:])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 71, in <lambda>\n",
            "    lambda s, l, t: Marker(s[t._original_start : t._original_end])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 278, in __init__\n",
            "    self._markers = _coerce_parse_result(MARKER.parseString(marker))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 143, in _coerce_parse_result\n",
            "    return [_coerce_parse_result(i) for i in results]\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1622, in _log\n",
            "    record = self.makeRecord(self.name, level, fn, lno, msg, args,\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1591, in makeRecord\n",
            "    rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 318, in __init__\n",
            "    self.module = os.path.splitext(self.filename)[0]\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 125, in splitext\n",
            "    return genericpath._splitext(p, sep, None, extsep)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# set globals\n",
        "\n",
        "# do Google Colab things\n",
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# install dependencies\n",
        "path_to_coderepo = (\n",
        "    \"/content/drive/MyDrive/ML_project/code_repo/flare-finder\" if IN_COLAB else \".\"\n",
        ")\n",
        "if IN_COLAB:\n",
        "    !pip install -r {path_to_coderepo}/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "DNmcTVnQ0MXA",
        "outputId": "6a9c0bc2-fd1e-4195-f3fe-6c03678001fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_main\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e9b431ad4eb6>\u001b[0m in \u001b[0;36m<cell line: 327>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0mbig_flare_finder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigFlareFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbig_flare_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_labels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;31m# make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e9b431ad4eb6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, image_paths, image_labels, val_frac)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mmodel_resnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# creating model class for big flare prediction\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import ResNet18_Weights\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "class BigFlareFinder:\n",
        "    def __init__(self):\n",
        "\n",
        "        # init pytorch model\n",
        "        self.pytorch_model = None\n",
        "\n",
        "        # set things to make training deterministic\n",
        "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "        torch.manual_seed(42)\n",
        "        torch.use_deterministic_algorithms(True)\n",
        "\n",
        "        # set device\n",
        "        self.device = None\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "    def fit(self, image_paths, image_labels, val_frac=0.5):\n",
        "\n",
        "        # # split the data into train-validation using sklearn and use stratified sampling\n",
        "        # image_paths_train, image_paths_val, image_labels_train, image_labels_val = (\n",
        "        #     train_test_split(\n",
        "        #         image_paths,\n",
        "        #         image_labels,\n",
        "        #         test_size=val_frac,\n",
        "        #         random_state=42,\n",
        "        #         stratify=image_labels,\n",
        "        #     )\n",
        "        # )\n",
        "\n",
        "        # split the data into train and validation using time\n",
        "        image_paths_train, image_paths_val, image_labels_train, image_labels_val = (\n",
        "            BigFlareFinder.split_into_train_test_by_time(image_paths, image_labels, val_frac)\n",
        "        )\n",
        "\n",
        "        # for training, augment minority-class by making copies\n",
        "        image_paths_train, image_labels_train = BigFlareFinder.augment_minority_class(\n",
        "            image_paths_train, image_labels_train\n",
        "        )\n",
        "\n",
        "        # get dataloader for train and validation data\n",
        "        train_loader = BigFlareFinder.preprocess(image_paths_train, image_labels_train)\n",
        "        validation_loader = BigFlareFinder.preprocess(image_paths_val, image_labels_val)\n",
        "\n",
        "        # fit pytorch model using dataloader\n",
        "\n",
        "        # load resnet18 model\n",
        "        # model_resnet18 = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
        "        model_resnet18 = torch.hub.load(\n",
        "            \"pytorch/vision\", \"resnet18\", weights=ResNet18_Weights.DEFAULT\n",
        "        )\n",
        "\n",
        "        # Freeze all params except the BatchNorm layers, as here they are trained to the\n",
        "        # mean and standard deviation of ImageNet and we may lose some signal\n",
        "        for name, param in model_resnet18.named_parameters():\n",
        "            if \"bn\" not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # reduce number of output classes in model\n",
        "        num_classes = 2\n",
        "        model_resnet18.fc = nn.Sequential(\n",
        "            nn.Linear(model_resnet18.fc.in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "        model_resnet18.to(self.device)\n",
        "        optimizer = optim.Adam(model_resnet18.parameters())\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "        epochs = 7  # 10\n",
        "        target_class = 1\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            training_loss = 0.0\n",
        "            valid_loss = 0.0\n",
        "            model_resnet18.train()\n",
        "            for batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                inputs, targets = batch\n",
        "                targets = targets.type(torch.LongTensor)\n",
        "                inputs = inputs.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "                output = model_resnet18(inputs)\n",
        "                loss = loss_fn(output, targets)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                training_loss += loss.data.item() * inputs.size(0)\n",
        "            training_loss /= len(train_loader.dataset)\n",
        "\n",
        "            model_resnet18.eval()\n",
        "            # all_targets = []\n",
        "            image_labels_val_pred = []\n",
        "\n",
        "            for batch in validation_loader:\n",
        "                inputs, targets = batch\n",
        "                inputs = inputs.to(self.device)\n",
        "                output = model_resnet18(inputs)\n",
        "                predictions = torch.max(F.softmax(output, dim=1), dim=1)[1]\n",
        "                image_labels_val_pred.extend(predictions.cpu().numpy())\n",
        "\n",
        "            print(\n",
        "                f\"Train epoch: {epoch}\"\n",
        "                f\", train_loss: {round(training_loss, 2)}\"\n",
        "                f\"\\nval_metrics: {BigFlareFinder.get_model_performance_metrics(image_labels_val, image_labels_val_pred)}\"\n",
        "            )\n",
        "\n",
        "        # train on the val data (which was excluded from training earlier)\n",
        "        image_paths_val, image_labels_val = BigFlareFinder.augment_minority_class(\n",
        "            image_paths_val, image_labels_val\n",
        "        )\n",
        "        val_loader = BigFlareFinder.preprocess(image_paths_val, image_labels_val)\n",
        "        for epoch in range(epochs):\n",
        "            for batch in val_loader:\n",
        "                optimizer.zero_grad()\n",
        "                inputs, targets = batch\n",
        "                targets = targets.type(torch.LongTensor)\n",
        "                inputs = inputs.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "                output = model_resnet18(inputs)\n",
        "                loss = loss_fn(output, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                training_loss += loss.data.item() * inputs.size(0)\n",
        "            training_loss /= len(train_loader.dataset)\n",
        "            model_resnet18.eval()\n",
        "            print(\n",
        "                f\"Train-on-val epoch: {epoch}\"\n",
        "                f\", train_loss: {round(training_loss, 2)}\"\n",
        "            )\n",
        "\n",
        "        # save trained model to self\n",
        "        self.pytorch_model = model_resnet18\n",
        "\n",
        "    @staticmethod\n",
        "    def augment_minority_class(image_paths, image_labels):\n",
        "        # init resut\n",
        "        image_paths_aug = []\n",
        "        image_labels_aug = []\n",
        "\n",
        "        # augment data\n",
        "        image_labels_counts = pd.Series(image_labels).value_counts().sort_values()\n",
        "        minority_class, majority_class = tuple(image_labels_counts.index)\n",
        "        class_count_diff = (\n",
        "            image_labels_counts[majority_class] - image_labels_counts[minority_class]\n",
        "        )\n",
        "        image_paths_new = (\n",
        "            pd.Series(image_paths[image_labels == minority_class])\n",
        "            .sample(class_count_diff, replace=True, random_state=42)\n",
        "            .to_list()\n",
        "        )\n",
        "        image_labels_new = [minority_class] * class_count_diff\n",
        "        image_paths_aug = image_paths + image_paths_new\n",
        "        image_labels_aug = image_labels + image_labels_new\n",
        "\n",
        "        # shuffle augmented data\n",
        "        image_paths_aug, image_labels_aug = zip(\n",
        "            *np.random.default_rng(seed=42).permutation(\n",
        "                list(zip(image_paths_aug, image_labels_aug))\n",
        "            )\n",
        "        )\n",
        "        image_paths_aug = [str(path) for path in image_paths_aug]\n",
        "        image_labels_aug = [float(label) for label in image_labels_aug]\n",
        "\n",
        "        return image_paths_aug, image_labels_aug\n",
        "\n",
        "    def predict(self, image_paths):\n",
        "\n",
        "        # init result\n",
        "        pred_labels = []\n",
        "\n",
        "        # get dataloader\n",
        "        dataloader = BigFlareFinder.preprocess(image_paths)\n",
        "\n",
        "        # make predictions\n",
        "        for batch in dataloader:\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(self.device)\n",
        "            output = self.pytorch_model(inputs)\n",
        "            predictions = torch.max(F.softmax(output, dim=1), dim=1)[1]\n",
        "            pred_labels.extend(predictions.cpu().numpy())\n",
        "\n",
        "        return pred_labels\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model_performance_metrics(y_true, y_pred):\n",
        "        metrics_dict = {\n",
        "            \"accuracy\": round(accuracy_score(y_true, y_pred), 2),\n",
        "            \"f1\": round(f1_score(y_true, y_pred), 2),\n",
        "            \"precision_class_1\": round(\n",
        "                precision_score(y_true, y_pred, zero_division=0), 2\n",
        "            ),\n",
        "            \"recall_class_1\": round(recall_score(y_true, y_pred, zero_division=0), 2),\n",
        "            \"actual_distru\": pd.Series(y_true).value_counts().sort_index().to_dict(),\n",
        "            \"pred_distru\": pd.Series(y_pred).value_counts().sort_index().to_dict(),\n",
        "        }\n",
        "        return metrics_dict\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(image_paths, image_labels=None):\n",
        "        # make dataset of image_paths and image_labels\n",
        "        image_dimension = 224\n",
        "        image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((image_dimension, image_dimension)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        image_labels = image_labels or [0] * len(image_paths)\n",
        "        dataset = CustomImageDataset(image_paths, image_labels, image_transforms)\n",
        "\n",
        "        # make dataloader for dataset\n",
        "        batch_size = 32\n",
        "        num_workers = 2\n",
        "        dataloader = DataLoader(\n",
        "            dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
        "        )\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    @staticmethod\n",
        "    def split_into_train_test_by_time(image_paths, image_labels, test_frac):\n",
        "        # make images_df\n",
        "        images_df = pd.DataFrame(\n",
        "            {\"image_path\": image_paths, \"image_label\": image_labels}\n",
        "        )\n",
        "\n",
        "        # add a datetime column\n",
        "        images_df[\"datetime\"] = pd.to_datetime(\n",
        "            images_df[\"image_path\"].str.split(\"/\").str[-1].str[None:-4]\n",
        "        )\n",
        "\n",
        "        # sort images_df by datetime\n",
        "        images_df = images_df.sort_values(\"datetime\")\n",
        "\n",
        "        # find min and max datetimes of images_df\n",
        "        min_datetime = images_df[\"datetime\"].min()\n",
        "        max_datetime = images_df[\"datetime\"].max()\n",
        "\n",
        "        # find time span between min and max datetimes (in days)\n",
        "        time_span_in_days = (max_datetime - min_datetime).days\n",
        "\n",
        "        # get the two dfs by splitting the time span into the desired ratio\n",
        "        num_train_days = time_span_in_days * (1 - test_frac)\n",
        "        end_train_datetime = min_datetime + pd.to_timedelta(num_train_days, unit=\"days\")\n",
        "        train_df = images_df[images_df[\"datetime\"] <= end_train_datetime]\n",
        "        test_df = images_df[images_df[\"datetime\"] > end_train_datetime]\n",
        "\n",
        "        # get image_paths_train, image_paths_test, image_labels_train, image_labels_test\n",
        "        image_paths_train = train_df[\"image_path\"].to_list()\n",
        "        image_paths_test = test_df[\"image_path\"].to_list()\n",
        "        image_labels_train = train_df[\"image_label\"].to_list()\n",
        "        image_labels_test = test_df[\"image_label\"].to_list()\n",
        "\n",
        "        return (\n",
        "            image_paths_train,\n",
        "            image_paths_test,\n",
        "            image_labels_train,\n",
        "            image_labels_test,\n",
        "        )\n",
        "\n",
        "\n",
        "# run the model on some data\n",
        "\n",
        "# get the image data: image_paths, image_labels\n",
        "image_folder_path = f\"{path_to_coderepo}/../../data/sdo_images\"\n",
        "big_flare_labels_df = pd.read_csv(f\"{path_to_coderepo}/big_flare_labels.csv\").dropna()\n",
        "\n",
        "# [TEMP] shorten big_flare_labels_df\n",
        "big_flare_labels_df = big_flare_labels_df.sort_values('solar_image_filename').iloc[None:2000]\n",
        "\n",
        "image_paths = (\n",
        "    image_folder_path + \"/\" + big_flare_labels_df[\"solar_image_filename\"]\n",
        ").to_list()\n",
        "image_labels = big_flare_labels_df[\"is_big_flare\"].to_list()\n",
        "\n",
        "# split image_paths and image_labels by time:\n",
        "#     image_paths_train, image_paths_test, image_labels_train, image_labels_test\n",
        "test_frac = 0.5\n",
        "image_paths_train, image_paths_test, image_labels_train, image_labels_test = (\n",
        "    BigFlareFinder.split_into_train_test_by_time(image_paths, image_labels, test_frac)\n",
        ")\n",
        "\n",
        "# train model\n",
        "big_flare_finder = BigFlareFinder()\n",
        "model = big_flare_finder.fit(image_paths_train, image_labels_train)\n",
        "\n",
        "# make predictions\n",
        "image_labels_test_pred = big_flare_finder.predict(image_paths_test)\n",
        "\n",
        "# get pred metrics\n",
        "test_metrics = BigFlareFinder.get_model_performance_metrics(image_labels_test, image_labels_test_pred)\n",
        "print(f\"\\ntest_metrics: {test_metrics}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dunZPQke0MXB"
      },
      "outputs": [],
      "source": [
        "# train big-flare classifier\n",
        "# * train big-flare classifier on first 70% of 2015 and test on last 30%\n",
        "# * show prediction-quality (accuracy, f1, precn, recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAmWCvgkS7je"
      },
      "outputs": [],
      "source": [
        "# show the timeline of flares in test along with predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yntWKplVS9qk"
      },
      "outputs": [],
      "source": [
        "# show what the model learned\n",
        "# * some viz of features picked up by the model for big-flares"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}